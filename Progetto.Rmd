---
title: "Text mining di alcuni articoli sulla guerra Russia-Ucraina"
author: "Alessio Pio Gennaro"
date: '2022-05-23'
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

Questa prima parte copre un aspetto molto importante dell'analisi, ovvero il processo di estrazione di testo da pagine web o documenti. Di solito è chiamato *Crawling* l'insieme di tecniche per il download automatico di pagine web.

Il processo di estrazione di dati testuali o metadati prende il nome di *Scraping*. Utilizziamo il package `rvest` con il supporto di [phantomJS](https://phantomjs.org/)

```{r}
# install.packages("rvest")
# install.packages("webdriver")

options(stringsAsFactors = F)

library(dplyr)
library(ggraph)
library(ggplot2)
library(igraph)
library(lubridate)
library(tidyr)
library(tidytext)
library(webdriver)
library(readr)
library(rvest)
library(stringr)
library(wordcloud)
library(widyr)

# install_phantomjs()

require(webdriver)

pjs_instance <- run_phantomjs("ERROR")
pjs_session <- Session$new(port = pjs_instance$port)

replace_na <- function(string) {
  if (is.na(string)) {
    return("")
  }
  return(string)
}

```

## Getting the feet wet

Facciamo subito un test.

```{r}
url <- "https://www.theguardian.com/world/2017/jun/26/angela-merkel-and-donald-trump-head-for-clash-at-g20-summit"

# caricamento dell'url nella sessione phantomJS
pjs_session$go(url)

# download del codice della pagina
rendered_source <- pjs_session$getSource()

# parsing del contenuto scaricato
html_document <- read_html(rendered_source)

html_document

# eventualmente
# html_document <- read_html(url)
```

******

Adesso che abbiamo a disposizione una pagina HTML cerchiamo di estrapolare informazioni dai tag.

[XPATH](https://en.wikipedia.org/wiki/XPath) è un linguaggio di interrogazione per selezionare elementi in strutture ad albero XML. Lo usiamo per selezionare l'elemento headline dalla pagina HTML. La seguente espressione *xpath* cerca gli elementi `h1`, in qualsiasi punto dell'albero `//` che soddisfano una certa condizione `[...]`, ovvero che l'attributo `class` dell'elemento `h1` deve contenere il valore `content__headline.`

```{r}
title_xpath <- "//div[contains(@data-gu-name, 'headline')]//h1"

title_text <- html_document %>%
  html_element(xpath = title_xpath) %>%
  html_text(trim = TRUE)

cat(title_text)
```

```{r}
intro_xpath <- "//div[contains(@data-gu-name, 'standfirst')]//p"

intro_text <- html_document %>%
  html_element(xpath = intro_xpath) %>%
  html_text(trim = TRUE)

cat(intro_text)
```

```{r}
body_xpath <- "//div[contains(@class, 'article-body')]//p"

body_text <- html_document %>%
  html_elements(xpath = body_xpath) %>%
  html_text(trim = TRUE) %>%
  paste0(collapse = "\n")

cat(body_text)

# typeof(body_text)
# class(body_text)
```

Infine aggiungiamo anche la data.

```{r}
# date_xpath <- "//time"
date_xpath <- "//meta[contains(@property, 'published_time')]"

date_object <- html_document %>%
  html_element(xpath = date_xpath) %>%
  html_attr(name = "content") %>%
  as.Date()

cat(format(date_object, "%Y-%m-%d"))

# typeof(date_object)
# class(date_object)
```

******

Di solito non vogliamo scaricare un solo documento ma una serie di documenti. Adesso vediamo come scaricare tutti gli articoli *taggati* con "Angela Merkel". Potremmo anche essere interessati a salvare i risultati di un motore di ricerca, o di una qualunque collezione di link.

```{r}
url <- "https://www.theguardian.com/world/angela-merkel"

pjs_session$go(url)
rendered_source <- pjs_session$getSource()
html_document <- read_html(rendered_source)
```

Adesso siamo interessati ad effettuare il download di ogni articolo *linkato*. Per farlo, estraiamo tutti gli attributi `href` dagli elementi `a` che rientrano in una *classe CSS*.

```{r}
links <- html_document %>% 
  html_elements(xpath = "//div[contains(@class, 'fc-item')]/a") %>% 
  html_attr(name = "href")

head(links, 5)
```

******

```{r eval=FALSE}
page_numbers <- 1:8
base_url <- "https://www.theguardian.com/world/angela-merkel?page="
paging_urls <- paste0(base_url, page_numbers)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//div[contains(@class, 'fc-item')]/a") %>% 
    html_attr(name = "href")
  
  all_links <- c(all_links, links)
}

head(all_links, 10)
length(all_links)

```

******

Combiniamo tutto in una funzione.

```{r}
scrape_guardian_article <- function(url) {
   
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)

  title_xpath <- "//div[contains(@data-gu-name, 'headline')]//h1"
  title_text <- html_document %>%
    html_element(xpath = title_xpath) %>%
    html_text(trim = TRUE)
  
  intro_xpath <- "//div[contains(@data-gu-name, 'standfirst')]//p"
  intro_text <- html_document %>%
    html_element(xpath = intro_xpath) %>%
    html_text(trim = TRUE)
  
  body_xpath <- "//div[contains(@class, 'article-body')]//p"
  body_text <- html_document %>%
    html_elements(xpath = body_xpath) %>%
    html_text(trim = TRUE) %>%
    paste0(collapse = "\n")
  
  date_xpath <- "//meta[contains(@property, 'published_time')]"
  date_object <- html_document %>%
    html_element(xpath = date_xpath) %>%
    html_attr(name = "content") %>%
    as.Date()
  
  article <- data.frame(
    url = url,
    date = date_object,
    title = title_text,
    body = paste0(intro_text, "\n", body_text)
  )
  
  article
}

scrape_all_guardian_articles <- function(all_links) {
  
  all_articles <- data.frame()
  
  for (i in 1:length(all_links)) {
    cat("Downloading", i, "of", length(all_links), "URL:", all_links[i], "\n")
    article <- scrape_guardian_article(all_links[i])
    
    all_articles <- all_articles %>% bind_rows(article)
  }
  
  all_articles
}
```

******

```{r eval=FALSE}
all_articles <- scrape_all_guardian_articles(all_links)
all_articles

write_csv2(all_articles, "./data/merkel_guardian.csv")
```

```{r}
all_articles <- read_csv2("data/merkel_guardian.csv")

all_articles
```

******

Adesso proviamo a fare la stessa cosa con un altro importante quotidiano, il [New York Times](https://www.nytimes.com).

```{r eval=FALSE}
nyt_base_url <- "https://www.nytimes.com"

add_base_url <- function(link) {
  
  if (!str_detect(link, paste0("^", nyt_base_url))) {
    link <- paste0(nyt_base_url, link)
  } else {
    link <- link
  }
  
  link
}

filter_links <- function(all_links) {
  
  filtered <- NULL
  
  for (link in links) {
    if (!str_detect(link, "/es/") &&
        !str_detect(link, "/video/")) {
      correct <- add_base_url(link = link)
      filtered <- c(filtered, correct)
    }
  }
  filtered
}

```

******

```{r eval=FALSE}
url <- "https://www.nytimes.com/topic/person/angela-merkel"

# download e parsing di una singola pagina web
pjs_session$go(url)
rendered_source <- pjs_session$getSource()
html_document <- read_html(rendered_source)

all_links <- NULL

link <- html_document %>% 
  html_elements(xpath = "//li[contains(@class, 'css-ye6x8s')]//a") %>%
  html_attr(name = "href")

all_links <- c(all_links, link)
all_links <- filter_links(all_links = all_links)

all_links
length(all_links)
```

******

Adesso passiamo ad analizzare il [Los Angeles Times](https://www.latimes.com/).

```{r}
page_numbers <- 1:16
base_url <- "https://www.latimes.com/search?q=Merkel&p="
paging_urls <- paste0(base_url, page_numbers)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//div[contains(@class, 'promo-wrapper')]//a") %>% 
    html_attr(name = "href")
  links <- unique(links)
  all_links <- c(all_links, links)
}

head(all_links, 10)
length(all_links)

```

```{r}
scrape_la_times_article <- function(url) {
   
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  title_xpath <- "//title"
  title_text <- html_document %>%
    html_element(xpath = title_xpath) %>%
    html_text(trim = TRUE)
  
  body_xpath <- "//div[contains(@class, 'page-article-body')]//p"
  body_text <- html_document %>%
    html_elements(xpath = body_xpath) %>%
    html_text(trim = TRUE) %>%
    paste0(collapse = "\n")
  
  date_xpath <- "//time[contains(@class, 'published-date')]"
  date_object <- html_document %>%
    html_element(xpath = date_xpath) %>%
    html_attr(name = "datetime") %>%
    as.Date()
  
  article <- data.frame(
    url = url,
    date = date_object,
    title = title_text,
    body = body_text
  )
  
  article
}

scrape_all_la_times_articles <- function(all_links) {
  
  all_articles <- data.frame()
  
  for (i in 1:length(all_links)) {
    cat("Downloading", i, "of", length(all_links), "URL:", all_links[i], "\n")
    article <- scrape_la_times_article(all_links[i])
    
    all_articles <- all_articles %>% bind_rows(article)
  }
  
  all_articles
}
```

```{r eval=FALSE}
all_articles <- scrape_all_la_times_articles(all_links)
all_articles

write_csv2(all_articles, "./data/merkel_la_times.csv")
```

```{r eval=FALSE}
all_articles <- read_csv2("data/merkel_la_times.csv")

all_articles
```

******

Un'altra fonte potrebbe essere il [New York Post](https://nypost.com). Replichiamo quanto fatto sopra.

```{r eval=FALSE}
page_numbers <- 1:8
base_url <- "https://nypost.com/search/Angela+Merkel/page/"
paging_urls <- paste0(base_url, page_numbers)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//a[contains(@class, 'postid')]") %>% 
    html_attr(name = "href")

  all_links <- c(all_links, links)
}

head(all_links, 10)
length(all_links)

```

******

Non abbiamo avuto molta fortuna. Un'altra sorgente da analizzare è il [Wall Street Journal](https://www.wsj.com).

```{r eval=FALSE}
page_numbers <- 1:9
base_url <- "https://www.wsj.com/search?query=Angela%20Merkel&mod=searchresults_viewallresults&page="
paging_urls <- paste0(base_url, page_numbers)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//article//div[contains(@class, 'WSJTheme--headline--7VCzo7Ay')]/h3/a") %>%
    html_attr(name = "href")

  all_links <- c(all_links, links)
}

head(all_links, 10)
length(all_links)

```

Tutti gli articoli sul sito web sono protetti da paywall. Niente da fare anche qui.

******

Proviamo con il [Metro](https://metro.co.uk).

```{r eval=FALSE}
page_numbers <- 1
base_url <- "https://metro.co.uk/search/#gsc.tab=0&gsc.q=Angela%20Merkel&gsc.sort=date&gsc.page="
paging_urls <- paste0(base_url, page_numbers)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//a[@class='gs-title']") %>%
    html_attr(name = "href")

  all_links <- c(all_links, links)
}

head(all_links, 10)
length(all_links)


```

******

Passiamo alla [CNN](https://cnn.com).

```{r eval=FALSE}
page_numbers <- 1
base_url <- "https://edition.cnn.com/search?q=Angela%20Merkel&size=10&sort=relevance&page="
paging_urls <- paste0(base_url, page_numbers)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//div[contains(@class, 'cnn-search__results-list')]//div")
    
  all_links <- c(all_links, links)
}

head(all_links, 10)
length(all_links)

all_links

```

******

Proviamo con il sito della [BBC](https://www.bbc.co.uk).

```{r}
page_numbers <- 1
base_url <- "https://www.bbc.co.uk/search?q=Angela+Merkel&page="
paging_urls <- paste0(base_url, page_numbers)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//a[contains(@class, 'ssrcss-1ynlzyd-PromoLink')]") %>%
    html_attr(name = "href")

  all_links <- c(all_links, links)
}

head(all_links, 10)
length(all_links)

```

```{r}
scrape_bbc_article <- function(url) {
   
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  title_xpath <- "//h1[contains(@id, 'main-heading')]"
  title_text1 <- html_document %>%
    html_element(xpath = title_xpath) %>%
    html_text(trim = TRUE)
  
  title_xpath <- "//h1[contains(@class, 'no-margin')]"
  title_text2 <- html_document %>%
    html_element(xpath = title_xpath) %>%
    html_text(trim = TRUE)
  
  title_text1 <- replace_na(title_text1)
  title_text2 <- replace_na(title_text2)
  title_text <- paste0(title_text1, title_text2)
  
  body_xpath <- "//div[contains(@class, 'RichTextContainer')]//p"
  body_text <- html_document %>%
    html_elements(xpath = body_xpath) %>%
    html_text(trim = TRUE) %>%
    paste0(collapse = "\n")
  
  body_xpath <- "//div[contains(@class, 'RichTextComponentWrapper')]//p"
  body_text2 <- html_document %>%
    html_elements(xpath = body_xpath) %>%
    html_text(trim = TRUE) %>%
    paste0(collapse = "\n")
  
  body_xpath <- "//div[contains(@class, 'synopsis-toggle__long')]//p"
  body_text3 <- html_document %>%
    html_elements(xpath = body_xpath) %>%
    html_text(trim = TRUE) %>%
    paste0(collapse = "\n")
  
  body_text = paste0(replace_na(body_text), replace_na(body_text2), replace_na(body_text3))
  
  date_xpath <- "//time[contains(@data-testid, 'timestamp')]"
  date_object <- html_document %>%
    html_element(xpath = date_xpath) %>%
    html_attr(name = "datetime") %>%
    as.Date()

    article <- data.frame(
    url = url,
    date = date_object,
    title = title_text,
    body = body_text
  )
  
  article
}

scrape_all_bbc_articles <- function(all_links) {
  
  all_articles <- data.frame()
  
  for (i in 1:length(all_links)) {
    cat("Downloading", i, "of", length(all_links), "URL:", all_links[i], "\n")
    article <- scrape_bbc_article(all_links[i])
    
    all_articles <- all_articles %>% bind_rows(article)
  }
  
  all_articles
}

```

```{r eval=FALSE}
all_articles <- scrape_all_bbc_articles(all_links)
all_articles

all_articles <- all_articles %>% 
  filter(!is.na(date))

write_csv2(all_articles, "./data/merkel_bbc.csv")
```

```{r eval=FALSE}
all_articles <- read_csv2("data/merkel_bbc.csv")

all_articles
```

******

Adesso tocca al [New Yorker](https://www.newyorker.com).

```{r}
page_numbers <- 1:18
begin_url <- "https://www.newyorker.com/search/q/Angela%20Merkel/page/"
end_url <- "/n,w"
paging_urls <- paste0(begin_url, page_numbers, end_url)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//div[contains(@class, 'River__riverItemContent___2hXMG')]/a[2]") %>% 
    html_attr(name = "href")
    
  all_links <- c(all_links, links)
}

head(all_links, 10)
length(all_links)
```

```{r}
scrape_new_yorker_article <- function(url) {
   
  pjs_session$go(paste0("https://www.newyorker.com", url))
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  title_xpath <- "//h1[contains(@data-testid, 'ContentHeaderHed')]"
  title_text <- html_document %>%
    html_element(xpath = title_xpath) %>%
    html_text(trim = TRUE)
  
  intro_xpath <- "//div[contains(@class, 'ContentHeaderDek')]"
  intro_text <- html_document %>%
    html_element(xpath = intro_xpath) %>%
    html_text(trim = TRUE)
  
  body_xpath <- "//p[contains(@class, 'paywall')]"
  body_text <- html_document %>%
    html_elements(xpath = body_xpath) %>%
    html_text(trim = TRUE) %>%
    paste0(collapse = "\n")

  date_xpath <- "//meta[contains(@property, 'article:published_time')]"
  date_object <- html_document %>%
    html_element(xpath = date_xpath) %>%
    html_attr(name = "content") %>%
    as.Date()
  
  article <- data.frame(
    url = paste0("https://www.newyorker.com", url),
    date = date_object,
    title = title_text,
    body = paste0(replace_na(intro_text), "\n", body_text)
  )
  
  article
}

scrape_all_new_yorker_articles <- function(all_links) {
  
  all_articles <- data.frame()
  
  for (i in 1:length(all_links)) {
    cat("Downloading", i, "of", length(all_links), "URL:", all_links[i], "\n")
    article <- scrape_new_yorker_article(all_links[i])
    
    all_articles <- all_articles %>% bind_rows(article)
  }
  
  all_articles
}

```

```{r eval=FALSE}
all_articles <- scrape_all_new_yorker_articles(all_links)
all_articles

write_csv2(all_articles, "./data/merkel_new_yorker.csv")
```

```{r eval=FALSE}
all_articles <- read_csv2("data/merkel_new_yorker.csv")
all_articles
```

******

Un'altra agenzia giornalistica riconosciuta a livello mondiale è [Reuters](https://www.reuters.com)

```{r eval=FALSE}
# page_numbers <- seq(0, 180, by = 10)
page_numbers <- 0
base_url <- "https://www.reuters.com/site-search/?query=Angela+Merkel&offset="
paging_urls <- paste0(base_url, page_numbers)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)

  # parsing ed estrazione dei link
  links <- html_document %>% 
    # html_elements(xpath = "//li[contains(@class, 'search-results__item__2oqiX')]")
    html_elements(xpath = "//div[contains(@class, 'search-layout__container__3HDMK')]")
    # html_elements(xpath = "//a") %>% html_attr(name = "href")
  
  all_links <- c(all_links, links)
}

head(all_links, 10)
length(all_links)

```

## Downloading articles about the Russia-Ucraine war

Siamo interessati agli articoli recenti che riguardano il conflitto Russia-Ucraina.

```{r eval=FALSE}
page_numbers <- 1:9
base_url <- "https://www.theguardian.com/world/ukraine?page="
paging_urls <- paste0(base_url, page_numbers)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//div[contains(@class, 'fc-item')]/a") %>% 
    html_attr(name = "href")
  
  all_links <- c(all_links, links)
}

head(all_links, 10)
length(all_links)

```

```{r eval=FALSE}
all_articles <- scrape_all_guardian_articles(all_links)
all_articles

write_csv2(all_articles, "./data/ukraine_guardian.csv")
```

```{r eval=FALSE}
all_articles <- read_csv2("data/ukraine_guardian.csv")

all_articles
```

******

```{r eval=FALSE}
page_numbers <- 1:20
base_url <- "https://www.latimes.com/search?q=Ukraine&p="
paging_urls <- paste0(base_url, page_numbers)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//h3[contains(@class, 'promo-title')]//a") %>%
    html_attr(name = "href")
  
  all_links <- c(all_links, links)
}

head(all_links, 10)
length(all_links)

```

```{r eval=FALSE}
all_articles <- scrape_all_la_times_articles(all_links)
all_articles

write_csv2(all_articles, "./data/ukraine_la_times.csv")
```

```{r eval=FALSE}
all_articles <- read_csv2("data/ukraine_la_times.csv")

all_articles
```

******

```{r eval=FALSE}
page_numbers <- 1:35
# page_numbers <- 1:10
base_url <- "https://www.bbc.co.uk/search?q=Ukraine&page="
paging_urls <- paste0(base_url, page_numbers)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//a[contains(@class, 'ssrcss-1ynlzyd-PromoLink')]") %>%
    html_attr(name = "href")

  all_links <- c(all_links, links)
}

all_links <- purrr::discard(all_links, function(x) {str_detect(x, "(programmes)|(sport)")})

head(all_links, 10)
length(all_links)

```

```{r eval=FALSE}
all_articles <- scrape_all_bbc_articles(all_links)
all_articles

write_csv2(all_articles, "./data/ukraine_bbc.csv")
```

```{r eval=FALSE}
all_articles <- read_csv2("./data/ukraine_bbc.csv")

all_articles
```

******

```{r eval=FALSE}
page_numbers <- 1:23
begin_url <- "https://www.newyorker.com/search/q/Ukraine/page/"
end_url <- "/n,w"
paging_urls <- paste0(begin_url, page_numbers, end_url)

head(paging_urls, 3)

all_links <- NULL

for (url in paging_urls) {
  # download e parsing di una singola pagina web
  pjs_session$go(url)
  rendered_source <- pjs_session$getSource()
  html_document <- read_html(rendered_source)
  
  # parsing ed estrazione dei link
  links <- html_document %>% 
    html_elements(xpath = "//div[contains(@class, 'River__riverItemContent___2hXMG')]/a[2]") %>% 
    html_attr(name = "href")
    
  all_links <- c(all_links, links)
}

all_links <- purrr::discard(all_links, function(x) {str_detect(x, "(podcast)|(humor)|(cartoons)|(sports)")})

head(all_links, 10)
length(all_links)
```

```{r eval=FALSE}
all_articles <- scrape_all_new_yorker_articles(all_links)
all_articles

write_csv2(all_articles, "data/ukraine_new_yorker.csv")
```

```{r eval=FALSE}
all_articles <- read_csv2("data/ukraine_new_yorker.csv")

all_articles
```

## Let's start the analysis

Adesso che abbiamo tutti gli articoli possiamo proseguire e dare inizio all'analisi vera e propria. L'analisi rispecchierà quanto già visto a lezione riguardo il text-mining.

## Pre-processing

```{r}
guardian <- read_csv2("data/ukraine_guardian.csv")

guardian <- guardian %>% 
  filter(!is.na(title), !is.na(date), !str_detect(body, "NA")) %>% 
  mutate(source = "Guardian") %>% 
  select(source, everything())

la_times <- read_csv2("data/ukraine_la_times.csv")

la_times <- la_times %>% 
  filter(!is.na(body))  %>% 
  mutate(source = "LA Times") %>% 
  select(source, everything())

bbc <- read_csv2("data/ukraine_bbc.csv")

bbc <- bbc %>% 
  filter(!is.na(title), !is.na(date), !str_detect(body, "NA")) %>% 
  mutate(source = "BBC") %>% 
  select(source, everything())

nyorker <- read_csv2("data/ukraine_new_yorker.csv")

nyorker <- nyorker %>% 
  filter(!is.na(date)) %>% 
  mutate(source = "New Yorker") %>% 
  select(source, everything())

all_articles <- guardian %>% 
  bind_rows(la_times) %>% 
  bind_rows(bbc) %>% 
  bind_rows(nyorker)

write_csv2(all_articles, "data/all_ukraine_articles.csv")

```

```{r}
all_articles <- read_csv2("./data/all_ukraine_articles.csv")
```

## Tidying

Una volta scaricati tutti gli articoli che ci interessano, siamo pronti ad eseguire l'analisi vera e propria. La prima cosa da fare è usare `unnest_tokens()` per dividere il testo in parole. Rimuoviamo anche le stop words.

```{r}
tidy_articles <- all_articles %>% 
  unnest_tokens(word, body)
tidy_articles

tidy_articles <- tidy_articles %>% 
  anti_join(stop_words)

tidy_articles %>% 
  count(word, sort = TRUE)
```

```{r}
tidy_articles %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 700) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

```{r}
tidy_articles %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))
```

```{r}
all_articles <- read_csv2("./data/all_ukraine_articles.csv")
```

## Sentiment analysis

Quando un umano legge un qualunque testo vi associa un sentimento, che viene ricavato dal significato delle parole. Non sempre il testo contiene emozioni positive o negative, possono anche esistere parti meno espressive. Useremo i tre _lexicon_ già visti a lezione.

```{r eval=FALSE}
get_sentiments("afinn") # da -5 a +5

```

```{r eval=FALSE}
get_sentiments("bing") # positive e negative

```

```{r eval=FALSE}
get_sentiments("nrc") # positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise e trust

```

*****

```{r}
tidy_articles <- all_articles %>% 
  unnest_tokens(word, body)

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_articles %>% 
  inner_join(nrc_joy) %>% 
  count(word, sort = TRUE)

```

Per ogni fonte, analizziamo il giorno con più articoli.

```{r}
tidy_articles_sentiment <- all_articles %>% 
  group_by(source) %>% 
  count(source, date, sort = TRUE) %>% 
  top_n(1) %>% 
  semi_join(all_articles, .) %>% 
  ungroup() %>% 
  unnest_tokens(word, body) %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(source, date, sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

tidy_articles_sentiment

```

```{r}
ggplot(tidy_articles_sentiment, aes(date, sentiment, fill = source)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~source, ncol = 2, scales = "free_x")

```

*****

Adesso confrontiamo i vari lexicon tra di loro. Utilizziamo solamente gli articoli della BBC.

```{r}
tidy_BBC <- tidy_articles %>% 
  filter(source == "BBC")
tidy_BBC

```

```{r}
afinn <- tidy_BBC %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(date) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")
afinn

bing_and_nrc <- bind_rows(
  tidy_BBC %>% 
    inner_join(get_sentiments("bing")) %>% 
    mutate(method = "Bing"),
  tidy_BBC %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", "negative"))
    ) %>% 
    mutate(method = "NRC")
  ) %>% 
  count(method, date, sentiment) %>% 
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
bing_and_nrc

```

```{r}
bind_rows(afinn,
          bing_and_nrc) %>% 
  ggplot(aes(date, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```

*****

Adesso possiamo prendere in considerazione le parole che hanno influito di più positivamente e negativamente.

```{r}
bing_word_counts <- tidy_articles %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()

bing_word_counts

```

Vediamo il bar plot.

```{r}
bing_word_counts %>% 
  group_by(sentiment) %>% 
  slice_max(n, n = 10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)

```

Visti i tanti significati della parola like, è meglio escluderla dall'analisi.

```{r}
custom_stop_words <- bind_rows(tibble(word = c("like"),
                                      lexicon = c("project")),
                               stop_words)

custom_stop_words

bing_word_counts <- bing_word_counts %>% 
  anti_join(custom_stop_words)
bing_word_counts

bing_word_counts %>% 
  group_by(sentiment) %>% 
  slice_max(n, n = 10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

## Word-cloud

Vediamo un word cloud in cui abbiamo diviso le parole in base al sentiment a loro associato.

```{r}
library(reshape2)

tidy_articles %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
  
```

## TF-IDF

La prossima misura che andremo ad analizzare è TF-IDF.

```{r}
articles_words <- all_articles %>% 
  unnest_tokens(word, body) %>% 
  count(source, word, sort = TRUE)

# numero totale di parole per ogni sorgente
total_words <- articles_words %>% 
  group_by(source) %>% 
  summarise(total = sum(n))

articles_words <- left_join(articles_words, total_words)

articles_words

```

```{r}
ggplot(articles_words, aes(n/total, fill = source)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~source, ncol = 2, scales = "free_y")

```

Come si nota facilmente, abbiamo tante parole che si incontrano molto raramente (la coda lunga) e un numero limitato di parole che si leggono molto spesso.

## Zipf law

La legge di Zipf studia il rapporto tra il rango e la frequenza delle parole di un testo. In particolare:

> La legge di Zipf afferma che la frequenza di apparizione di una parola è inversamente proporzionale al suo rango.

Dove per rango si intende la posizione che la parola occupa in una classificazione delle parole basata sulla frequenza.
La legge di Zipf ha una distribuzione Power-Law.

$$p_k = \alpha k^{-1}$$

$$p_k$$ è la frequenza della parola. $$k$$, invece, rappresenta il rango. $$\alpha$$ è una costante.

```{r}
freq_by_rank <- articles_words %>% 
  group_by(source) %>% 
  mutate(rank = row_number(),
         `term frequency` = n/total)

freq_by_rank

```

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, group = source, color = source)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = TRUE) +
  scale_x_log10() +
  scale_y_log10()

```

*****

La porzione di rango che va da 100 a 1000 sembra costante. Analizziamo questa sezione.

```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 1000,
         rank > 100)

mod = lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
mod

```

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = source)) + 
  geom_abline(intercept = mod$coefficients[1], 
              slope = mod$coefficients[2], 
              color = "gray50", 
              linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

```

*****

Torniamo ad analizzare TF-IDF.

```{r}
articles_tf_idf <- articles_words %>% 
  bind_tf_idf(word, source, n)

articles_tf_idf

articles_tf_idf %>% 
  select(-total) %>% 
  arrange(desc(tf_idf))

```

```{r}
articles_tf_idf %>% 
  group_by(source) %>% 
  slice_max(tf_idf, n = 15) %>% 
  ungroup() %>% 
  ggplot(aes(tf_idf, reorder(word, tf_idf), fill = source)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~source, ncol = 2, scales = "free") +
  labs(x = "TF-IDF", y = NULL)

```

## Bigram analysis

Come già fatto a lezione, procediamo ad analizzare i bigrammi, cioè gruppi di due parole consecutive. Come vedremo, i bigrammi ci torneranno particolarmente utili nella sentiment analysis, in quanto permettono di identificare le negazioni nel testo.

```{r}
articles_bigrams <- all_articles %>% 
  unnest_tokens(bigram, body, token = "ngrams", n = 2)

articles_bigrams

```

```{r}
articles_bigrams %>% 
  count(bigram, sort = TRUE)

```

Rimuoviamo le stop words.

```{r}
bigrams_separated <- articles_bigrams %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

```

```{r}
bigrams_united <- bigrams_filtered %>% 
  unite(bigram, word1, word2, sep = " ")

bigrams_united

```

## Uno sguardo ai trigrammi

Possiamo anche dare uno sguardo a come volge l'analisi nel caso in cui si considerano trigrammi.

```{r}
all_articles %>% 
  unnest_tokens(trigram, body, token = "ngrams", n = 3) %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>% 
  count(word1, word2, word3, sort = TRUE)

```

```{r}
bigrams_filtered %>% 
  filter(word1 == "zelensky") %>% 
  count(source, word2, sort = TRUE)

```

Diamo un'occhiata a TF-IDF, considerando un bigramma come fosse una parola.

```{r}
bigram_tf_idf <- bigrams_united %>% 
  count(source, bigram) %>% 
  bind_tf_idf(bigram, source, n) %>% 
  arrange(desc(tf_idf))

bigram_tf_idf %>% 
  group_by(source) %>% 
  slice_max(tf_idf, n = 10) %>% 
  ungroup() %>% 
  ggplot(aes(tf_idf, reorder(bigram, tf_idf), fill = source)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~source, ncol = 2, scales = "free") +
  labs(x = "TF-IDF", y = NULL)

```

## Bigram sentiment analysis

```{r}
bigrams_separated %>% 
  filter(word1 == "not") %>% 
  count(word1, word2, sort = TRUE)

```

```{r}
not_words <- bigrams_separated %>% 
  filter(word1 == "not") %>% 
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>% 
  count(word2, value, sort = TRUE)

not_words

```

```{r}
not_words %>% 
  mutate(contribution = n * value) %>% 
  arrange(desc(abs(contribution))) %>% 
  head(20) %>% 
  mutate(word2 = reorder(word2, contribution)) %>% 
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * num of occurrences",
       y = "Words preceded by \"not\"")

```

## Negative words

```{r}
negation_words <- c("no", "not", "none", "noone", "nobody", "nothing", "neither", "nowhere", "never", "without")

negated_words <- bigrams_separated %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>% 
  count(word1, word2, value, sort = TRUE)
negated_words

negated_words %>% 
  group_by(word1) %>% 
  count() %>% 
  filter(n >= 10) %>% 
  semi_join(negated_words, ., by = "word1") %>%
  ungroup() %>%
  mutate(contribution = n * value) %>% 
  arrange(desc(abs(contribution))) %>% 
  group_by(word1) %>% 
  top_n(contribution, n = 10) %>% 
  ungroup() %>% 
  mutate(word2 = reorder(word2, contribution)) %>% 
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~word1, ncol = 2, scales = "free_y") +
  labs(x = "Sentiment value * num of occurrences",
       y = "Words preceded by a negation")

```

## Graph representation

Possiamo utilizzare la libreria [ggraph](https://www.github.com/thomasp85/ggraph) (e il pacchetto [igraph](https://www.igraph.org/)) per visualizzare le relazioni tra coppie di parole.

```{r}
# teniamo solamente le coppie più comuni
bigram_graph <- bigram_counts %>% 
  filter(n >= 40) %>% 
  graph_from_data_frame()

bigram_graph

```

```{r}
set.seed(927)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```

Nel grafo si possono notare tante coppie di parole separate tra di loro (come nuclear - weapons, front - line, black - sea, etc.) e una grossa componente connessa.
Parole come "ukrainian", "russian", "city" e "war" rappresentano centri comuni e vanno in coppia con tantissime altre parole.

Segue un'altra visualizzazione del grafo.

```{r}
set.seed(927)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```

## Co-occurrencies and other statistics

Usiamo il package [widyr](https://github.com/dgrtwo/widyr) per esplorare le co-occorrenze che avvengono tra documenti o parti di questi.

```{r}
all_articles_words <- all_articles %>%
  filter(source == "BBC") %>% 
  # gruppi di dieci articoli
  mutate(group = row_number() %/% 10) %>% 
  unnest_tokens(word, body) %>% 
  filter(!word %in% stop_words$word)

all_articles_words

```

```{r}
word_pairs <- all_articles_words %>% 
  pairwise_count(word, group, sort = TRUE)

word_pairs

```

*****

Adesso prendiamo in considerazione la **correlazione** tra parole, che indica quanto spesso due parole appaiono insieme rispetto a quanto appaiono separate.

Il coefficiente phi indica quanto è probabile che le due parole (X e Y) appaiano insieme (o non appaiano insieme) rispetto a quanto una compaia senza l'altra.

|  | Y appare | Y non appare | Totale |  |
|------------|---------------|---------------|--------------|---|
| X appare | $n_{11}$ | $n_{10}$ | $n_{1\cdot}$ |  |
| X non appare | $n_{01}$ | $n_{00}$ | $n_{0\cdot}$ |  |
| Totale | $n_{\cdot 1}$ | $n_{\cdot 0}$ | $n$ |  |

$$\phi=\frac{n_{11}n_{00}-n_{10}n_{01}}{\sqrt{n_{1\cdot}n_{0\cdot}n_{\cdot0}n_{\cdot1}}}$$

```{r}
word_cor <- all_articles_words %>% 
  group_by(word) %>% 
  filter(n() >= 40) %>% 
  pairwise_cor(word, group, sort = TRUE)

word_cor

```

Le parole più correlate a "weapons"

```{r}
word_cor %>% 
  filter(item1 == "weapons")

```

*****

Vediamo lo stesso risultato anche per altre parole.

```{r}
word_cor %>%
  filter(item1 %in% c("putin", "zelensky", "kremlin", "tanks", "russians", "sanctions")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~item1, scales = "free") +
  coord_flip()

```

Segue anche il grafo delle correlazioni. Valori di correlazione più alti sono indicati da un valore alto dell'attributo *edge_alpha*.

```{r}
set.seed(927)

word_cor %>%
  filter(correlation > 0.95) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

















